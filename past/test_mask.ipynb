{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch import einsum\n",
    "from einops import  reduce\n",
    "\n",
    "\n",
    "class CategoricalMasked(Categorical):\n",
    "    def __init__(self, logits: torch.Tensor, mask: Optional[torch.Tensor] = None):\n",
    "        self.mask = mask\n",
    "        self.batch, self.nb_action = logits.size()\n",
    "        if mask is None:\n",
    "            super(CategoricalMasked, self).__init__(logits=logits)\n",
    "        else:\n",
    "            self.mask_value = torch.tensor(\n",
    "                torch.finfo(logits.dtype).min, dtype=logits.dtype\n",
    "            )\n",
    "            logits = torch.where(self.mask, logits, self.mask_value)\n",
    "            super(CategoricalMasked, self).__init__(logits=logits)\n",
    "\n",
    "    def entropy(self):\n",
    "        if self.mask is None:\n",
    "            return super().entropy()\n",
    "        # Elementwise multiplication\n",
    "        p_log_p = einsum(\"ij,ij->ij\", self.logits, self.probs)\n",
    "        # Compute the entropy with possible action only\n",
    "        p_log_p = torch.where(\n",
    "            self.mask,\n",
    "            p_log_p,\n",
    "            torch.tensor(0, dtype=p_log_p.dtype, device=p_log_p.device),\n",
    "        )\n",
    "        return -reduce(p_log_p, \"b a -> b\", \"sum\", b=self.batch, a=self.nb_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_extended_shape', '_get_checked_instance', '_new', '_validate_args', '_validate_sample', 'arg_constraints', 'batch_shape', 'cdf', 'entropy', 'enumerate_support', 'event_shape', 'expand', 'has_enumerate_support', 'has_rsample', 'icdf', 'log_prob', 'logits', 'mean', 'mode', 'param_shape', 'perplexity', 'probs', 'rsample', 'sample', 'sample_n', 'set_default_validate_args', 'stddev', 'support', 'variance']\n",
      "tensor([1.0114, 1.0114])\n",
      "tensor([[-1.7918, -1.0986, -0.6931],\n",
      "        [-1.7918, -1.0986, -0.6931]])\n",
      "tensor([[0.1667, 0.3333, 0.5000],\n",
      "        [0.1667, 0.3333, 0.5000]])\n"
     ]
    }
   ],
   "source": [
    "a = Categorical\n",
    "print(dir(a))\n",
    "\n",
    "b = a(torch.tensor([[1.0, 2.0, 3.0], [1.0, 2.0, 3.0]]))\n",
    "print(b.entropy())\n",
    "print(b.logits)\n",
    "print(b.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1594, 0.0198, 0.8208, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0430, 0.0455, 0.9115, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class CategoricalMasking(Categorical):\n",
    "    def __init__(self, logits: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        self.mask = mask\n",
    "        self.batch, self.nb_action = logits.size()\n",
    "        self.mask_value = torch.tensor(\n",
    "            torch.finfo(logits.dtype).min, dtype=logits.dtype\n",
    "        )\n",
    "        logits = torch.where(self.mask, logits, self.mask_value)\n",
    "        super(CategoricalMasking, self).__init__(logits=logits)\n",
    "\n",
    "    def entropy(self):\n",
    "        # Elementwise multiplication\n",
    "        p_log_p = einsum(\"ij,ij->ij\", self.logits, self.probs)\n",
    "        # Compute the entropy with possible action only\n",
    "        p_log_p = torch.where(\n",
    "            self.mask,\n",
    "            p_log_p,\n",
    "            torch.tensor(0, dtype=p_log_p.dtype, device=p_log_p.device),\n",
    "        )\n",
    "        return -reduce(p_log_p, \"b a -> b\", \"sum\", b=self.batch, a=self.nb_action)\n",
    "\n",
    "\n",
    "logits = torch.randn(2, 100, requires_grad=True)\n",
    "mask = torch.zeros(2, 100, dtype=torch.bool)\n",
    "mask[:, 0] = True\n",
    "mask[:, 1] = True\n",
    "mask[:, 2] = True\n",
    "\n",
    "dist = CategoricalMasking(logits = logits, mask = mask)\n",
    "\n",
    "print(dist.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2366e-01, -7.6685e-01,  2.4841e-01,  ...,  8.3988e-01,\n",
      "          2.2024e+00,  6.5887e-01],\n",
      "        [-5.1119e-01, -1.0158e+00, -1.6483e-03,  ...,  6.6559e-01,\n",
      "         -5.9711e-02,  5.7094e-01]], requires_grad=True)\n",
      "tensor([[False, False,  True,  ..., False, False, False],\n",
      "        [ True,  True, False,  ..., False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "logits_or_qvalues = torch.randn((2, 10000), requires_grad=True) # batch size, nb action\n",
    "print(logits_or_qvalues) \n",
    "\n",
    "\n",
    "mask = torch.zeros((2, 10000), dtype=torch.bool) # batch size, nb action\n",
    "mask[0][2] = True\n",
    "mask[1][0] = True\n",
    "mask[1][1] = True\n",
    "print(mask) # False -> mask action \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.7861e-05, 2.7853e-05, 7.6876e-05,  ..., 1.3889e-04, 5.4248e-04,\n",
      "         1.1589e-04],\n",
      "        [3.6736e-05, 2.2179e-05, 6.1149e-05,  ..., 1.1917e-04, 5.7699e-05,\n",
      "         1.0841e-04]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.6235, 0.3765, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([8.7067, 8.7181], grad_fn=<NegBackward0>)\n",
      "tensor([-0.0000, 0.6623], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "head = CategoricalMasked(logits=logits_or_qvalues)\n",
    "print(head.probs) # Impossible action are not masked\n",
    "# tensor([[0.0447, 0.8119, 0.1434], There remain 3 actions available\n",
    "#         [0.2745, 0.6353, 0.0902]]) There remain 3 actions available\n",
    "\n",
    "head_masked = CategoricalMasked(logits=logits_or_qvalues, mask=mask)\n",
    "print(head_masked.probs) # Impossible action are  masked\n",
    "# tensor([[0.0000, 0.0000, 1.0000], There remain 1 actions available\n",
    "#         [0.3017, 0.6983, 0.0000]]) There remain 2 actions available\n",
    "\n",
    "print(head.entropy())\n",
    "# tensor([0.5867, 0.8601])\n",
    "\n",
    "print(head_masked.entropy())\n",
    "# tensor([-0.0000, 0.6123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13552"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7*44*44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
